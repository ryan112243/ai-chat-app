#!/usr/bin/env python3
"""
AIæ¨¡å‹å„ªåŒ–å™¨
ä½¿ç”¨ç”Ÿæˆçš„è¨“ç·´è³‡æ–™é€²è¡Œæ¨¡å‹å¾®èª¿å’Œå„ªåŒ–
"""

import json
import os
import asyncio
from typing import List, Dict, Any
from training_data_manager import TrainingExample, training_manager
from ai_service import ai_service

class ModelOptimizer:
    def __init__(self):
        self.training_data_dir = "./training_data/processed"
        self.optimization_results = []
        
    def load_all_training_data(self) -> List[TrainingExample]:
        """è¼‰å…¥æ‰€æœ‰è¨“ç·´è³‡æ–™"""
        all_data = []
        
        if not os.path.exists(self.training_data_dir):
            print("âŒ è¨“ç·´è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨")
            return all_data
            
        for filename in os.listdir(self.training_data_dir):
            if filename.endswith('.json'):
                try:
                    data = training_manager.load_training_data(filename)
                    all_data.extend(data)
                    print(f"âœ… è¼‰å…¥ {filename}: {len(data)} å€‹ç¯„ä¾‹")
                except Exception as e:
                    print(f"âŒ è¼‰å…¥ {filename} å¤±æ•—: {str(e)}")
        
        print(f"\nğŸ“Š ç¸½å…±è¼‰å…¥ {len(all_data)} å€‹è¨“ç·´ç¯„ä¾‹")
        return all_data
    
    async def evaluate_model_performance(self, test_examples: List[TrainingExample]) -> Dict[str, float]:
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ” è©•ä¼°æ¨¡å‹æ€§èƒ½...")
        
        correct_predictions = 0
        total_predictions = len(test_examples)
        domain_performance = {}
        
        for i, example in enumerate(test_examples[:50]):  # æ¸¬è©¦å‰50å€‹ç¯„ä¾‹
            try:
                # ä½¿ç”¨AIæœå‹™ç”Ÿæˆå›æ‡‰
                response = await ai_service.generate_response(
                    message=example.user_input,
                    domain=example.domain
                )
                
                # ç°¡å–®çš„æ€§èƒ½è©•ä¼°ï¼ˆåŸºæ–¼å›æ‡‰é•·åº¦å’Œé—œéµè©åŒ¹é…ï¼‰
                is_good_response = self._evaluate_response_quality(
                    example.ai_response, 
                    response.content if hasattr(response, 'content') else str(response)
                )
                
                if is_good_response:
                    correct_predictions += 1
                
                # æŒ‰é ˜åŸŸçµ±è¨ˆ
                if example.domain not in domain_performance:
                    domain_performance[example.domain] = {'correct': 0, 'total': 0}
                
                domain_performance[example.domain]['total'] += 1
                if is_good_response:
                    domain_performance[example.domain]['correct'] += 1
                
                if (i + 1) % 10 == 0:
                    print(f"  é€²åº¦: {i + 1}/50")
                    
            except Exception as e:
                print(f"  è©•ä¼°ç¯„ä¾‹ {i+1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        # è¨ˆç®—æ•´é«”æº–ç¢ºç‡
        overall_accuracy = correct_predictions / min(50, total_predictions) if total_predictions > 0 else 0
        
        # è¨ˆç®—å„é ˜åŸŸæº–ç¢ºç‡
        domain_accuracies = {}
        for domain, stats in domain_performance.items():
            domain_accuracies[domain] = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
        
        results = {
            'overall_accuracy': overall_accuracy,
            'domain_accuracies': domain_accuracies,
            'total_evaluated': min(50, total_predictions)
        }
        
        return results
    
    def _evaluate_response_quality(self, expected: str, actual: str) -> bool:
        """è©•ä¼°å›æ‡‰å“è³ª"""
        if not actual or len(actual.strip()) < 10:
            return False
        
        # åŸºæœ¬å“è³ªæª¢æŸ¥
        expected_words = set(expected.lower().split())
        actual_words = set(actual.lower().split())
        
        # è¨ˆç®—è©å½™é‡ç–Šç‡
        overlap = len(expected_words.intersection(actual_words))
        overlap_ratio = overlap / len(expected_words) if expected_words else 0
        
        # é•·åº¦æª¢æŸ¥
        length_ratio = len(actual) / len(expected) if expected else 1
        
        # ç¶œåˆè©•åˆ†
        quality_score = (overlap_ratio * 0.6) + (min(length_ratio, 2.0) / 2.0 * 0.4)
        
        return quality_score > 0.3
    
    async def optimize_model_parameters(self, training_data: List[TrainingExample]):
        """å„ªåŒ–æ¨¡å‹åƒæ•¸"""
        print("\nâš™ï¸ å„ªåŒ–æ¨¡å‹åƒæ•¸...")
        
        # åˆ†æè¨“ç·´è³‡æ–™ç‰¹å¾µ
        domain_stats = {}
        for example in training_data:
            domain = example.domain
            if domain not in domain_stats:
                domain_stats[domain] = {
                    'count': 0,
                    'avg_input_length': 0,
                    'avg_output_length': 0,
                    'complexity_scores': []
                }
            
            stats = domain_stats[domain]
            stats['count'] += 1
            stats['avg_input_length'] += len(example.user_input)
            stats['avg_output_length'] += len(example.ai_response)
            
            # è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸
            complexity = self._calculate_complexity(example.user_input)
            stats['complexity_scores'].append(complexity)
        
        # è¨ˆç®—å¹³å‡å€¼
        for domain, stats in domain_stats.items():
            if stats['count'] > 0:
                stats['avg_input_length'] /= stats['count']
                stats['avg_output_length'] /= stats['count']
                stats['avg_complexity'] = sum(stats['complexity_scores']) / len(stats['complexity_scores'])
        
        # åŸºæ–¼çµ±è¨ˆè³‡æ–™èª¿æ•´AIæœå‹™åƒæ•¸
        optimization_config = self._generate_optimization_config(domain_stats)
        
        print("ğŸ“ˆ å„ªåŒ–é…ç½®:")
        for domain, config in optimization_config.items():
            print(f"  â€¢ {domain}: {config}")
        
        return optimization_config
    
    def _calculate_complexity(self, text: str) -> float:
        """è¨ˆç®—æ–‡æœ¬è¤‡é›œåº¦"""
        words = text.split()
        sentences = text.split('.')
        
        # åŸºæœ¬è¤‡é›œåº¦æŒ‡æ¨™
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        avg_sentence_length = sum(len(sentence.split()) for sentence in sentences) / len(sentences) if sentences else 0
        
        # ç‰¹æ®Šå­—ç¬¦å’Œæ•¸å­—çš„æ¯”ä¾‹
        special_chars = sum(1 for char in text if not char.isalnum() and not char.isspace())
        special_ratio = special_chars / len(text) if text else 0
        
        complexity = (avg_word_length * 0.3) + (avg_sentence_length * 0.5) + (special_ratio * 100 * 0.2)
        return min(complexity, 10.0)  # é™åˆ¶åœ¨0-10ç¯„åœå…§
    
    def _generate_optimization_config(self, domain_stats: Dict) -> Dict[str, Dict]:
        """ç”Ÿæˆå„ªåŒ–é…ç½®"""
        config = {}
        
        for domain, stats in domain_stats.items():
            # åŸºæ–¼çµ±è¨ˆè³‡æ–™èª¿æ•´åƒæ•¸
            complexity = stats.get('avg_complexity', 5.0)
            
            domain_config = {
                'temperature': max(0.1, min(1.0, complexity / 10.0)),
                'max_tokens': int(stats['avg_output_length'] * 1.2) if stats['avg_output_length'] > 0 else 500,
                'top_p': 0.9 if complexity > 6 else 0.8,
                'frequency_penalty': 0.1 if domain == 'writing' else 0.0
            }
            
            config[domain] = domain_config
        
        return config
    
    async def run_optimization(self):
        """åŸ·è¡Œå®Œæ•´çš„æ¨¡å‹å„ªåŒ–æµç¨‹"""
        print("ğŸš€ é–‹å§‹AIæ¨¡å‹å„ªåŒ–...")
        print("=" * 60)
        
        # 1. è¼‰å…¥è¨“ç·´è³‡æ–™
        training_data = self.load_all_training_data()
        if not training_data:
            print("âŒ æ²’æœ‰å¯ç”¨çš„è¨“ç·´è³‡æ–™")
            return
        
        # 2. è©•ä¼°ç•¶å‰æ¨¡å‹æ€§èƒ½
        test_data = training_data[:100]  # ä½¿ç”¨å‰100å€‹ä½œç‚ºæ¸¬è©¦é›†
        performance = await self.evaluate_model_performance(test_data)
        
        print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½è©•ä¼°çµæœ:")
        print(f"  â€¢ æ•´é«”æº–ç¢ºç‡: {performance['overall_accuracy']:.2%}")
        print(f"  â€¢ è©•ä¼°ç¯„ä¾‹æ•¸: {performance['total_evaluated']}")
        
        print(f"\nğŸ“ˆ å„é ˜åŸŸæ€§èƒ½:")
        for domain, accuracy in performance['domain_accuracies'].items():
            print(f"  â€¢ {domain}: {accuracy:.2%}")
        
        # 3. å„ªåŒ–æ¨¡å‹åƒæ•¸
        optimization_config = await self.optimize_model_parameters(training_data)
        
        # 4. ä¿å­˜å„ªåŒ–çµæœ
        results = {
            'timestamp': asyncio.get_event_loop().time(),
            'performance': performance,
            'optimization_config': optimization_config,
            'training_data_count': len(training_data)
        }
        
        self._save_optimization_results(results)
        
        print("\nâœ… æ¨¡å‹å„ªåŒ–å®Œæˆï¼")
        print("ğŸ’¡ å»ºè­°å®šæœŸé‡æ–°åŸ·è¡Œå„ªåŒ–ä»¥æŒçºŒæ”¹å–„æ¨¡å‹æ€§èƒ½")
        
        return results
    
    def _save_optimization_results(self, results: Dict):
        """ä¿å­˜å„ªåŒ–çµæœ"""
        results_dir = "./optimization_results"
        os.makedirs(results_dir, exist_ok=True)
        
        filename = f"optimization_results_{int(asyncio.get_event_loop().time())}.json"
        filepath = os.path.join(results_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å„ªåŒ–çµæœå·²ä¿å­˜åˆ°: {filepath}")

# å…¨åŸŸå„ªåŒ–å™¨å¯¦ä¾‹
model_optimizer = ModelOptimizer()

async def main():
    """ä¸»å‡½æ•¸"""
    await model_optimizer.run_optimization()

if __name__ == "__main__":
    asyncio.run(main())